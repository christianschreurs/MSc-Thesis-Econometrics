{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6c63069",
   "metadata": {},
   "source": [
    "# Thesis Macroeconomic effect of Trump\n",
    "## Table of Contents\n",
    "* [Explanation](#chapter0)\n",
    "* [1. Data Generating Process](#chapter1)\n",
    "    * [1.1. Import Packages](#section1_1)\n",
    "    * [1.2. Set Initial Parameters](#section1_2)\n",
    "    * [1.3. Define DGP Functions](#section1_3)\n",
    "    * [1.4. Example Data plot](#section1_4)\n",
    "* [2. Synthetic Control Class definitions and functions](#chapter2)\n",
    "    * [2.1 Synthetic Control Class definition](#section2_1)\n",
    "    * [2.1 Help functions simulation](#section2_2)\n",
    "* [3. Simulation](#chapter3)\n",
    "    * [3.1. Different Variables](#section3_1)\n",
    "    * [3.2. Different V Matrices](#section3_2)\n",
    "    * [3.3. Different Weight Constraints](#section3_3)\n",
    "\n",
    "## Explanation <a class=\"anchor\" id=\"chapter0\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf4e434",
   "metadata": {},
   "source": [
    "## 1. Data Generating Process <a class=\"anchor\" id=\"chapter1\"></a>\n",
    "### 1.1. Import Packages <a class=\"anchor\" id=\"section1_1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afded665",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from scipy.optimize import minimize\n",
    "from cvxopt import solvers, matrix\n",
    "solvers.options['show_progress'] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e8ad6f",
   "metadata": {},
   "source": [
    "### 1.2. Set Initial Parameters <a class=\"anchor\" id=\"section1_2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41ca02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DGP parameters\n",
    "J = 20 # No. of countries, J-1 no. of donor countries\n",
    "T = 25 # Total periods\n",
    "T0 = 15 # Pre-treatment period\n",
    "K = 10\n",
    "\n",
    "# Simulation parameters\n",
    "TE_sizes = [0, 0.1, 0.25, 0.5, 1] # Real treatment effects\n",
    "n_sims = 1000\n",
    "\n",
    "# SCM parameters\n",
    "maxiter = 1000\n",
    "maxfev = 1000\n",
    "t0 = 10 # Cross-validation parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca803eb",
   "metadata": {},
   "source": [
    "### 1.3. Define DGP function <a class=\"anchor\" id=\"section1_3\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd3648d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dgp(J, T0, T, process, treatmenteffect=1000):\n",
    "    \n",
    "    # Parameter to generate our AR processes\n",
    "    rho = 0.5\n",
    "    sd_rho = np.sqrt(1-rho**2)\n",
    "    \n",
    "    # Create unit identifiers (names)\n",
    "    unit_ids = [chr(j + 64) for j in range(1, J+1)]\n",
    "\n",
    "    # Create an empty dataframe\n",
    "    data = pd.DataFrame(np.nan, index=np.arange(T), columns=unit_ids)\n",
    "\n",
    "    # Create the common time trend\n",
    "    delta = np.random.normal(loc=0, scale=1, size=T)\n",
    "\n",
    "    # Create the 10 stationary components\n",
    "    lambda_mat = np.zeros((T, 10))\n",
    "    for k in range(10):\n",
    "        lambda_mat[:, k] = np.random.normal(loc=rho, scale=sd_rho, size=T)\n",
    "        \n",
    "    # Create the 2 non-stationary components\n",
    "    phi_mat = np.random.normal(loc=0, scale=1, size=(T, 2))\n",
    "    for t in range(1, T):\n",
    "        phi_mat[t, :] += phi_mat[t-1, :]    \n",
    "\n",
    "    # Create the observed outcome\n",
    "    for j in range(1, J+1):\n",
    "        \n",
    "        # Group indicator of the stationary component\n",
    "        mu1 = np.zeros(10)\n",
    "        mu1[(j-1)//2] = 1\n",
    "        \n",
    "        # Group indicator of the non-stationary component\n",
    "        mu2 = np.zeros(2)\n",
    "        mu2[j // 11] = 1\n",
    "        \n",
    "        # Error term\n",
    "        if process == 'stationary':\n",
    "            epsilon = np.random.normal(loc=0, scale=1, size=T)\n",
    "        elif process == 'nonstationary':\n",
    "            epsilon = np.random.normal(loc=0, scale=1, size=T)\n",
    "        \n",
    "        # Observed outcome\n",
    "        if process == 'stationary':\n",
    "            data[unit_ids[j-1]] = delta + np.dot(lambda_mat, mu1) + epsilon\n",
    "        elif process == 'nonstationary':\n",
    "            data[unit_ids[j-1]] = delta + np.dot(lambda_mat, mu1) + np.dot(phi_mat, mu2) + epsilon\n",
    "        \n",
    "    data[unit_ids[0]][T0:] += treatmenteffect*(np.arange(T0,T)-T0)\n",
    "        \n",
    "    y = data.iloc[:,0].values.reshape(T,1)\n",
    "    X = data.iloc[:,1:]\n",
    "        \n",
    "    return data, y, X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379e78f0",
   "metadata": {},
   "source": [
    "### 1.4. Example Data plot <a class=\"anchor\" id=\"section1_4\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce64c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_stationary, y_stationary, X_stationary = dgp(J, T0, T, 'stationary', 1)\n",
    "data_nonstationary, y_nonstationary, X_nonstationary = dgp(J, T0, T, 'nonstationary', 1)\n",
    "placebo_units = data_stationary.columns.unique()\n",
    "\n",
    "plt.figure(figsize=(16,5))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(data_stationary['A'], color='blue')\n",
    "plt.plot(data_stationary.drop(['A'], axis=1), color='grey', alpha=.5)\n",
    "plt.axvline(T0, color='grey', linestyle='dashed', alpha=.75)\n",
    "plt.title('Stationary DGP')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(data_nonstationary['A'], color='blue')\n",
    "plt.plot(data_nonstationary.drop(['A'], axis=1), color='grey', alpha=.5)\n",
    "plt.axvline(T0, color='grey', linestyle='dashed', alpha=.75)\n",
    "plt.title('Nonstationary DGP');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07153c7a",
   "metadata": {},
   "source": [
    "## 2. Synthetic Control Class definition and functions <a class=\"anchor\" id=\"chapter2\"></a>\n",
    "### 2.1 Synthetic Control Class definition  <a class=\"anchor\" id=\"section2_1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e54a5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SyntheticControl:\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        outer_loop_method='Nelder-Mead',\n",
    "        maxiter=maxiter,\n",
    "        maxfev=maxfev,\n",
    "        xatol=1e-3,\n",
    "        fatol=1e-12,\n",
    "        verbose_outer=True,\n",
    "        verbose_inner=True,\n",
    "        weight_nonnegative_restr=True,\n",
    "        weight_sum_restr=True,\n",
    "        count=0,\n",
    "        include_intercept=False,\n",
    "        demean=False,\n",
    "    ):\n",
    "        self.outer_loop_method = outer_loop_method\n",
    "        self.maxiter = maxiter\n",
    "        self.maxfev = maxfev\n",
    "        self.xatol = xatol\n",
    "        self.fatol = fatol\n",
    "        self.verbose_outer = verbose_outer\n",
    "        self.verbose_inner = verbose_inner\n",
    "        self.weight_nonnegative_restr = weight_nonnegative_restr\n",
    "        self.weight_sum_restr = weight_sum_restr\n",
    "        self.count = count\n",
    "        self.include_intercept = include_intercept\n",
    "        self.demean = demean\n",
    "        \n",
    "    def _loss_function(self, v2, y, X):\n",
    "\n",
    "        # Initialize matrices\n",
    "        V = np.diag(np.insert(np.exp(v2), 0, 1))\n",
    "        H = np.dot(X.T, np.dot(V, X))\n",
    "        l = X.shape[1]\n",
    "\n",
    "        # Create correct CVxOPT matrices for quad. optimization loop with constraints\n",
    "        Q = matrix((H + H.T) / 2)\n",
    "        r = matrix((-np.dot(y.T, np.dot(V, X))).T)\n",
    "        G = matrix(-np.eye(l))\n",
    "        h = matrix(np.zeros(l))\n",
    "        A = matrix(np.ones(l)).T\n",
    "        b = matrix(1.0)\n",
    "\n",
    "        # Optimization\n",
    "        if not self.weight_nonnegative_restr:\n",
    "            G, h = None, None\n",
    "        if not self.weight_sum_restr:\n",
    "            A, b = None, None\n",
    "\n",
    "        w = solvers.qp(Q, r, G, h, A, b, kktsolver='ldl')[\"x\"]\n",
    "        SSR = np.sum((y - np.dot(X, w)) ** 2)\n",
    "\n",
    "        # Print progress\n",
    "        self.count += 1\n",
    "        if self.verbose_inner and self.count % 1000 == 0:\n",
    "            print(f\"Function evaluation {self.count}/{self.maxfev} \\tloss = {SSR}\")\n",
    "\n",
    "        return SSR\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "\n",
    "        # Set starting values for outer loop\n",
    "        s = np.hstack((y, X)).std(axis=1)\n",
    "        s1, s2 = s[0], s[1:]\n",
    "        v20 = np.log((s1/s2)**2)\n",
    "        minimize_options = {'maxiter': self.maxiter, \n",
    "                            'maxfev': self.maxfev,\n",
    "                            'disp': self.verbose_outer,\n",
    "                            'xatol': self.xatol, \n",
    "                            'fatol': self.fatol}\n",
    "        \n",
    "        result = minimize(self._loss_function, v20, args=(y, X), method=self.outer_loop_method, \n",
    "              options = minimize_options)\n",
    "        v2 = result.x\n",
    "        \n",
    "        # Restore weights from V weights\n",
    "        self.V = np.diag(np.insert(np.exp(v2), 0, 1))\n",
    "        H = np.dot(X.T, np.dot(self.V, X))\n",
    "        l = np.shape(X)[1]\n",
    "        \n",
    "        # Transform to correct CVxOPT types\n",
    "        Q = matrix((H + H.T)/2)\n",
    "        r = matrix((-np.dot(y.T, np.dot(self.V, X))).T)\n",
    "        G = matrix(-np.eye(l))\n",
    "        h = matrix(np.zeros(l))\n",
    "        A = matrix(np.ones(l)).T   \n",
    "        b = matrix(1.0)\n",
    "        \n",
    "        # Optimization and resulting weights\n",
    "        if not self.weight_nonnegative_restr:\n",
    "            G, h  = None, None\n",
    "        if not self.weight_sum_restr:\n",
    "            A, b = None, None\n",
    "        \n",
    "        self.weights = np.array(solvers.qp(Q, r, G, h, A, b, kktsolver='ldl')['x']).T\n",
    "\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a0e8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SyntheticControlVAR:\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        weight_nonnegative_restr = True,\n",
    "        weight_sum_restr = True,\n",
    "        verbose_inner = False,\n",
    "        verbose_outer = False,\n",
    "    ):\n",
    "        self.weight_nonnegative_restr = weight_nonnegative_restr\n",
    "        self.weight_sum_restr = weight_sum_restr\n",
    "        self.verbose_inner = verbose_inner\n",
    "        self.verbose_outer = verbose_outer\n",
    "       \n",
    "    def fit(self, X, y):\n",
    "        # Find number of donor countries\n",
    "        self.n_donorcountries = np.shape(X)[1]\n",
    "\n",
    "        # Define V matrix\n",
    "        self.V = np.diag(1/np.var(X, axis=1))\n",
    "\n",
    "        # Set up optimization for quadratic programming\n",
    "        H = np.dot(X.T, np.dot(self.V, X))\n",
    "        l = np.shape(X)[1]\n",
    "\n",
    "        # Transform to correct CVxOPT types\n",
    "        Q = matrix((H + H.T)/2)\n",
    "        r = matrix((-np.dot(y.T, np.dot(self.V,X))).T)\n",
    "        G = matrix(-np.eye(l))\n",
    "        h = matrix(np.zeros(l))\n",
    "        A = matrix(np.ones(l)).T\n",
    "        b = matrix(1.0)\n",
    "\n",
    "        # Optimization and resulting weights\n",
    "        if not self.weight_nonnegative_restr:\n",
    "            G, h  = None, None\n",
    "        if not self.weight_sum_restr:\n",
    "            A, b = None, None\n",
    "\n",
    "        self.weights = np.array(solvers.qp(Q, r, G, h, A, b, kktsolver='ldl')[\"x\"]).T[0]\n",
    "\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f494b9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SyntheticControlCV:\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        outer_loop_method = 'Nelder-Mead',\n",
    "        maxiter = maxiter,\n",
    "        maxfev = maxfev,\n",
    "        xatol = 1e-3,\n",
    "        fatol = 1e-12,\n",
    "        verbose_outer = True,\n",
    "        verbose_inner = True,\n",
    "        weight_nonnegative_restr = True,\n",
    "        weight_sum_restr = True,\n",
    "        count = 0,\n",
    "    ):\n",
    "        self.outer_loop_method = outer_loop_method\n",
    "        self.maxiter = maxiter\n",
    "        self.maxfev = maxfev\n",
    "        self.xatol = xatol\n",
    "        self.fatol = fatol\n",
    "        self.verbose_outer = verbose_outer\n",
    "        self.verbose_inner = verbose_inner\n",
    "        self.weight_nonnegative_restr = weight_nonnegative_restr\n",
    "        self.weight_sum_restr = weight_sum_restr\n",
    "        self.count = count\n",
    "        \n",
    "    def _loss_function(self, v2, y_train, X_train, y_validation, X_validation):\n",
    "        \n",
    "        # Initalize matrices\n",
    "        V = np.diag(np.insert(np.exp(v2), 0, 1))\n",
    "        H = np.dot(X_train.T, np.dot(V, X_train))\n",
    "        l = np.shape(X_train)[1]\n",
    "\n",
    "        # Create correct CVxOPT matrices for quad. optimization loop with constraints\n",
    "        Q = matrix((H + H.T)/2)\n",
    "        r = matrix((-np.dot(y_train.T, np.dot(V, X_train))).T)\n",
    "        G = matrix(-np.eye(l))\n",
    "        h = matrix(np.zeros(l))\n",
    "        A = matrix(np.ones(l)).T\n",
    "        b = matrix(1.0)\n",
    "\n",
    "        # Optimization\n",
    "        if not self.weight_nonnegative_restr:\n",
    "            G, h  = None, None\n",
    "        if not self.weight_sum_restr:\n",
    "            A, b = None, None\n",
    "\n",
    "        w = solvers.qp(Q, r, G, h, A, b)['x']\n",
    "        SSR = np.sum((y_validation - np.dot(X_validation, w))**2)\n",
    "\n",
    "        # Print progress\n",
    "        self.count += 1\n",
    "        if (self.verbose_inner) & (self.count%1000==0):\n",
    "            print(f'Function evaluation {self.count}/{self.maxfev} \\tloss = {SSR}')\n",
    "\n",
    "        return SSR\n",
    "\n",
    "    def fit(self, X, y):\n",
    "\n",
    "        # 1. Divide the pre-intervention periods into a initial training and validation period\n",
    "        # Training set contains 1-10, validation set 11-15\n",
    "        y_train = y_nonstationary[:10]\n",
    "        X_train = X_nonstationary.iloc[:t0, :].values\n",
    "        y_validation = y_nonstationary[t0:T0]\n",
    "        X_validation = X_nonstationary.iloc[t0:T0, :].values\n",
    "\n",
    "        # Set starting values for outer loop\n",
    "        s = np.hstack((y_train, X_train)).std(axis=1)\n",
    "        s1, s2 = s[0], s[1:]\n",
    "        v20 = np.log((s1/s2)**2)\n",
    "        minimize_options = {'maxiter': self.maxiter, \n",
    "                            'maxfev': self.maxfev,\n",
    "                            'disp': self.verbose_outer,\n",
    "                            'xatol': self.xatol, \n",
    "                            'fatol': self.fatol}\n",
    "\n",
    "        # Outer loop\n",
    "        result = minimize(self._loss_function, v20, args=(y_train, X_train, y_validation, X_validation), \n",
    "                          method=self.outer_loop_method, options = minimize_options)\n",
    "        v2 = result.x\n",
    "\n",
    "        # Restore weights from V weights\n",
    "        self.V = np.diag(np.insert(np.exp(v2), 0, 1))\n",
    "        H = np.dot(X_train.T, np.dot(self.V, X_train))\n",
    "        l = np.shape(X_train)[1]\n",
    "\n",
    "        # Transform to correct CVxOPT types\n",
    "        Q = matrix((H + H.T)/2)\n",
    "        r = matrix((-np.dot(y_train.T, np.dot(self.V, X_train))).T)\n",
    "        G = matrix(-np.eye(l))\n",
    "        h = matrix(np.zeros(l))\n",
    "        A = matrix(np.ones(l)).T\n",
    "        b = matrix(1.0)\n",
    "\n",
    "        # Optimization and resulting weights\n",
    "        if not self.weight_nonnegative_restr:\n",
    "            G, h  = None, None\n",
    "        if not self.weight_sum_restr:\n",
    "            A, b = None, None\n",
    "\n",
    "        self.weights = np.array(solvers.qp(Q, r, G, h, A, b)['x']).T\n",
    "\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8bb998",
   "metadata": {},
   "source": [
    "### 2.2. Help functions simulation <a class=\"anchor\" id=\"section2_2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2138424d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gap_computer(data_X, placebo_unit, w):\n",
    "    df_treated = data_X[placebo_unit]\n",
    "    df_untreated = data_X.drop(placebo_unit, axis=1)\n",
    "    doppelganger = df_untreated.dot(w.T)\n",
    "    result = (df_treated.values - doppelganger.values.flatten())\n",
    "    \n",
    "    return result\n",
    "\n",
    "def placebo_weight_and_gap_computer(data_X, method):\n",
    "    gap_dict = dict()\n",
    "    for placebo_unit in placebo_units:\n",
    "        y = data_X[placebo_unit][:T0].values.reshape(T0, 1)\n",
    "        X = data_X.iloc[:T0, data_X.columns != placebo_unit].values\n",
    "        \n",
    "        SCplacebo = method.fit(X, y)\n",
    "        w = SCplacebo.weights\n",
    "\n",
    "        gap_dict[placebo_unit] = gap_computer(data_X, placebo_unit, w)\n",
    "        \n",
    "    return gap_dict\n",
    "\n",
    "def test_statistic(gap_dict, sign=None):\n",
    "    if sign == 'positive':\n",
    "        gap_dict_positive = np.clip(gap_dict, 0, None)\n",
    "        gap_sum = np.sum(gap_dict_positive**2)\n",
    "    elif sign == 'negative':\n",
    "        gap_dict_negative = np.clip(gap_dict, None, 0)\n",
    "        gap_sum = np.sum(gap_dict_negative**2)\n",
    "    else:\n",
    "        gap_sum = np.sum(gap_dict**2)\n",
    "        \n",
    "    return gap_sum/len(gap_dict)\n",
    "\n",
    "def ratiofit_pre_post(gap_dict):\n",
    "    avg_ratiofit_countrydict = dict()\n",
    "    pre_treatment_fits = dict()\n",
    "    post_treatment_fits = dict()\n",
    "    \n",
    "    for placebo_unit in placebo_units:\n",
    "        pre_treatment_fit = test_statistic(gap_dict[placebo_unit][:(T0+1)])\n",
    "        post_treatment_fit = test_statistic(gap_dict[placebo_unit][(T0+1):], 'positive')\n",
    "        \n",
    "        pre_treatment_fits[placebo_unit] = pre_treatment_fit\n",
    "        post_treatment_fits[placebo_unit] = post_treatment_fit\n",
    " \n",
    "        avg_ratio = post_treatment_fit/pre_treatment_fit\n",
    "        avg_ratiofit_countrydict[placebo_unit] = avg_ratio\n",
    "           \n",
    "    return avg_ratiofit_countrydict\n",
    "\n",
    "def SA1_placebo_weight_and_gap_computer(data_X, y_X_variable, X_X_variable, method, \n",
    "                                        y2_X_variable=None, X2_X_variable=None):\n",
    "    gap_dict = dict()\n",
    "    for placebo_unit in placebo_units:\n",
    "        y = data_X[placebo_unit][:T0].values.reshape(T0, 1)\n",
    "        X = data_X.iloc[:T0, data_X.columns != placebo_unit]\n",
    "        \n",
    "        new_y = np.vstack([y, y_X_variable])\n",
    "        new_X = np.vstack([X.values, X_X_variable.values])\n",
    "        \n",
    "        if y2_X_variable is not None:\n",
    "            new_y = np.vstack([new_y, y2_X_variable])\n",
    "            new_X = np.vstack([new_X, X2_X_variable.values])\n",
    "\n",
    "        SCplacebo = method.fit(new_X, new_y)\n",
    "        w = SCplacebo.weights\n",
    "\n",
    "        gap_dict[placebo_unit] = gap_computer(data_X, placebo_unit, w)\n",
    "\n",
    "    return gap_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557f46e6",
   "metadata": {},
   "source": [
    "## 3. Simulation <a class=\"anchor\" id=\"chapter3\"></a>\n",
    "## 3.1. Different Variables <a class=\"anchor\" id=\"section3_1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9299f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "variable_results = []\n",
    "for sim_no in tqdm(range(n_sims)):\n",
    "    for TE_size in TE_sizes:\n",
    "        \n",
    "        ###### Generate data\n",
    "        data_stationary, y_stationary, X_stationary = dgp(J, T0, T, 'stationary', TE_size)\n",
    "        data_nonstationary, y_nonstationary, X_nonstationary = dgp(J, T0, T, 'nonstationary', TE_size)\n",
    "        \n",
    "        data_S_variable, y_S_variable, X_S_variable = dgp(J, T0, T0, 'stationary')\n",
    "        data_N_variable, y_N_variable, X_N_variable = dgp(J, T0, T0, 'nonstationary')\n",
    "        \n",
    "        ###### Stationary data\n",
    "        # Original method\n",
    "        original_gap_dict = placebo_weight_and_gap_computer(\n",
    "            data_stationary, SyntheticControl(verbose_inner=False, verbose_outer=False))\n",
    "        original_te = np.round(np.sum(original_gap_dict['A'][(T0+1):])/45, 2)\n",
    "        original_p_value = (20 - (pd.Series(ratiofit_pre_post(original_gap_dict).values()).rank())[0])/19\n",
    "        \n",
    "        # Append stationary variable\n",
    "        plus_s_gap_dict = SA1_placebo_weight_and_gap_computer(\n",
    "            data_stationary, y_S_variable, X_S_variable,\n",
    "            SyntheticControl(verbose_inner=False, verbose_outer=False))\n",
    "        plus_s_te = np.round(np.sum(plus_s_gap_dict['A'][(T0+1):])/45, 2)\n",
    "        plus_s_p_value = (20 - (pd.Series(ratiofit_pre_post(plus_s_gap_dict).values()).rank())[0])/19\n",
    "        \n",
    "        # Append nonstationary variable\n",
    "        plus_n_gap_dict = SA1_placebo_weight_and_gap_computer(\n",
    "            data_stationary, y_N_variable, X_N_variable,\n",
    "            SyntheticControl(verbose_inner=False, verbose_outer=False))\n",
    "        plus_n_te = np.round(np.sum(plus_n_gap_dict['A'][(T0+1):])/45, 2)\n",
    "        plus_n_p_value = (20 - (pd.Series(ratiofit_pre_post(plus_n_gap_dict).values()).rank())[0])/19\n",
    "            \n",
    "        # Append stationary and nonstationary variables\n",
    "        plus_sn_gap_dict = SA1_placebo_weight_and_gap_computer(\n",
    "            data_stationary, y_S_variable, X_S_variable,\n",
    "            SyntheticControl(verbose_inner=False, verbose_outer=False),\n",
    "            y_N_variable, X_N_variable)\n",
    "        plus_sn_te = np.round(np.sum(plus_sn_gap_dict['A'][(T0+1):])/45, 2)\n",
    "        plus_sn_p_value = (20 - (pd.Series(ratiofit_pre_post(plus_sn_gap_dict).values()).rank())[0])/19\n",
    "        \n",
    "        ###### Nonstationary data\n",
    "        # Original method\n",
    "        N_original_gap_dict = placebo_weight_and_gap_computer(\n",
    "            data_nonstationary, SyntheticControl(verbose_inner=False, verbose_outer=False))\n",
    "        N_original_te = np.round(np.sum(N_original_gap_dict['A'][(T0+1):])/45, 2)\n",
    "        N_original_p_value = (20 - (pd.Series(ratiofit_pre_post(N_original_gap_dict).values()).rank())[0])/19\n",
    "        \n",
    "        # Append stationary variable\n",
    "        N_plus_s_gap_dict = SA1_placebo_weight_and_gap_computer(\n",
    "            data_nonstationary, y_S_variable, X_S_variable,\n",
    "            SyntheticControl(verbose_inner=False, verbose_outer=False))\n",
    "        N_plus_s_te = np.round(np.sum(N_plus_s_gap_dict['A'][(T0+1):])/45, 2)\n",
    "        N_plus_s_p_value = (20 - (pd.Series(ratiofit_pre_post(N_plus_s_gap_dict).values()).rank())[0])/19\n",
    "        \n",
    "        # Append nonstationary variable\n",
    "        N_plus_n_gap_dict = SA1_placebo_weight_and_gap_computer(\n",
    "            data_nonstationary, y_N_variable, X_N_variable,\n",
    "            SyntheticControl(verbose_inner=False, verbose_outer=False))\n",
    "        N_plus_n_te = np.round(np.sum(N_plus_n_gap_dict['A'][(T0+1):])/45, 2)\n",
    "        N_plus_n_p_value = (20 - (pd.Series(ratiofit_pre_post(N_plus_n_gap_dict).values()).rank())[0])/19\n",
    "        \n",
    "        # Append stationary and nonstationary variables\n",
    "        N_plus_sn_gap_dict = SA1_placebo_weight_and_gap_computer(\n",
    "            data_nonstationary, y_S_variable, X_S_variable,\n",
    "            SyntheticControl(verbose_inner=False, verbose_outer=False),\n",
    "            y_N_variable, X_N_variable,)\n",
    "        N_plus_sn_te = np.round(np.sum(N_plus_sn_gap_dict['A'][(T0+1):])/45, 2)\n",
    "        N_plus_sn_p_value = (20 - (pd.Series(ratiofit_pre_post(N_plus_sn_gap_dict).values()).rank())[0])/19\n",
    "        \n",
    "        # Store TE and p-values\n",
    "        variable_results.append({'Simulation': sim_no+1, 'TE_size': TE_size,\n",
    "                                 'S TE': original_te, 'S p-val':original_p_value,\n",
    "                                 'S+S TE': plus_s_te, 'S+S p-val':plus_s_p_value,\n",
    "                                 'S+N TE': plus_n_te, 'S+N p-val':plus_n_p_value,\n",
    "                                 'S+S+N TE': plus_sn_te, 'S+S+N p-val':plus_sn_p_value,\n",
    "                                 'N TE': N_original_te, 'N p-val':N_original_p_value,\n",
    "                                 'N+S TE': N_plus_s_te, 'N+S p-val':N_plus_s_p_value,\n",
    "                                 'N+N TE': N_plus_n_te, 'N+N p-val':N_plus_n_p_value,\n",
    "                                 'N+S+N TE': N_plus_sn_te, 'N+S+N p-val':N_plus_sn_p_value\n",
    "                                })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5de5a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_variable_results = pd.DataFrame(variable_results)\n",
    "np.round(df_variable_results.groupby(['TE_size']).mean().drop('Simulation', axis=1), 3).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b7f3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_variable_results_to_latex = np.round(\n",
    "    df_variable_results.groupby(['TE_size']).mean().drop('Simulation', axis=1), 3).T\n",
    "print(df_variable_results_to_latex.style.to_latex())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bbdc56a",
   "metadata": {},
   "source": [
    "## 3.2. Different V Matrices <a class=\"anchor\" id=\"section3_2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b86dde9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "V_results = []\n",
    "for sim_no in tqdm(range(n_sims)):\n",
    "    for TE_size in TE_sizes:\n",
    "        \n",
    "        ###### Generate data\n",
    "        data_stationary, y_stationary, X_stationary = dgp(J, T0, T, 'stationary', TE_size)\n",
    "        data_nonstationary, y_nonstationary, X_nonstationary = dgp(J, T0, T, 'nonstationary', TE_size)\n",
    "\n",
    "        ###### Stationary data\n",
    "        # Original method\n",
    "        try:\n",
    "            original_gap_dict = placebo_weight_and_gap_computer(\n",
    "                data_stationary, SyntheticControl(verbose_inner=False, verbose_outer=False))\n",
    "            original_te = np.round(np.sum(original_gap_dict['A'][(T0+1):])/45, 2)\n",
    "            original_p_value = (20 - (pd.Series(ratiofit_pre_post(original_gap_dict).values()).rank())[0])/19\n",
    "        except:\n",
    "            original_te, original_p_value = np.nan, np.nan\n",
    "        \n",
    "        # Inverted Variance method\n",
    "        try:\n",
    "            invv_gap_dict = placebo_weight_and_gap_computer(\n",
    "                data_stationary, SyntheticControlVAR(verbose_inner=False, verbose_outer=False))\n",
    "            invv_te = np.round(np.sum(invv_gap_dict['A'][(T0+1):])/45, 2)\n",
    "            invv_p_value = (20 - (pd.Series(ratiofit_pre_post(invv_gap_dict).values()).rank())[0])/19\n",
    "        except:\n",
    "            invv_te, invv_p_value = np.nan, np.nan\n",
    "        \n",
    "        # Cross-validation method\n",
    "        try:\n",
    "            cv_gap_dict = placebo_weight_and_gap_computer(\n",
    "                data_stationary, SyntheticControlCV(verbose_inner=False, verbose_outer=False))\n",
    "            cv_te = np.round(np.sum(cv_gap_dict['A'][(T0+1):])/45, 2)\n",
    "            cv_p_value = (20 - (pd.Series(ratiofit_pre_post(cv_gap_dict).values()).rank())[0])/19\n",
    "        except:\n",
    "            cv_te, cv_p_value = np.nan, np.nan\n",
    "        \n",
    "        ###### Nonstationary data\n",
    "        # Original method\n",
    "        try:\n",
    "            N_original_gap_dict = placebo_weight_and_gap_computer(\n",
    "                data_nonstationary, SyntheticControl(verbose_inner=False, verbose_outer=False))\n",
    "            N_original_te = np.round(np.sum(N_original_gap_dict['A'][(T0+1):])/45, 2)\n",
    "            N_original_p_value = (20 - (pd.Series(ratiofit_pre_post(N_original_gap_dict).values()).rank())[0])/19\n",
    "        except:\n",
    "            N_original_te, N_original_p_value = np.nan, np.nan\n",
    "        \n",
    "        # Inverted Variance method\n",
    "        try:\n",
    "            N_invv_gap_dict = placebo_weight_and_gap_computer(\n",
    "                data_nonstationary, SyntheticControlVAR(verbose_inner=False, verbose_outer=False))\n",
    "            N_invv_te = np.round(np.sum(N_invv_gap_dict['A'][(T0+1):])/45, 2)\n",
    "            N_invv_p_value = (20 - (pd.Series(ratiofit_pre_post(N_invv_gap_dict).values()).rank())[0])/19\n",
    "        except:\n",
    "            N_invv_te, N_invv_p_value = np.nan, np.nan\n",
    "        \n",
    "        # Cross-validation method\n",
    "        try:\n",
    "            N_cv_gap_dict = placebo_weight_and_gap_computer(\n",
    "                data_nonstationary, SyntheticControlCV(verbose_inner=False, verbose_outer=False))\n",
    "            N_cv_te = np.round(np.sum(N_cv_gap_dict['A'][(T0+1):])/45, 2)\n",
    "            N_cv_p_value = (20 - (pd.Series(ratiofit_pre_post(N_cv_gap_dict).values()).rank())[0])/19\n",
    "        except:\n",
    "            N_cv_te, N_cv_p_value = np.nan, np.nan\n",
    "        \n",
    "        # Store TE and p-values\n",
    "        V_results.append({'Simulation': sim_no+1, 'TE_size': TE_size,\n",
    "                          'O TE': original_te, 'O p-val':original_p_value,\n",
    "                          'IV TE': invv_te, 'IV p-val':invv_p_value,\n",
    "                          'CV TE': cv_te, 'CV p-val':cv_p_value,\n",
    "                          'N. O TE': N_original_te, 'N. O p-val':N_original_p_value,\n",
    "                        'N. IV TE': N_invv_te, 'N. IV p-val':N_invv_p_value,\n",
    "                        'N. CV TE': N_cv_te, 'N. CV p-val':N_cv_p_value,\n",
    "                       })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d07e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_V_results = pd.DataFrame(V_results)\n",
    "np.round(df_V_results.groupby(['TE_size']).mean().drop('Simulation', axis=1), 3).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b99a5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_V_to_latex = np.round(df_V_results.groupby(['TE_size']).mean().drop('Simulation', axis=1),3).T\n",
    "print(df_V_to_latex.style.to_latex())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43173fb5",
   "metadata": {},
   "source": [
    "## 3.3. Different Weight Constraints <a class=\"anchor\" id=\"section3_3\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016ecf16",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sims=10\n",
    "W_results = []\n",
    "for sim_no in tqdm(range(n_sims)):\n",
    "    for TE_size in TE_sizes:\n",
    "        \n",
    "        ###### Generate data\n",
    "        data_stationary, y_stationary, X_stationary = dgp(J, T0, T, 'stationary', TE_size)\n",
    "        data_nonstationary, y_nonstationary, X_nonstationary = dgp(J, T0, T, 'nonstationary', TE_size)\n",
    "\n",
    "        ###### Stationary data\n",
    "        # Original method\n",
    "        try:\n",
    "            original_gap_dict = placebo_weight_and_gap_computer(\n",
    "                data_stationary, SyntheticControl(\n",
    "                    verbose_inner=False, verbose_outer=False, weight_nonnegative_restr=True, weight_sum_restr=True))\n",
    "            original_te = np.round(np.sum(original_gap_dict['A'][(T0+1):])/45, 2)\n",
    "            original_p_value = (20 - (pd.Series(ratiofit_pre_post(original_gap_dict).values()).rank())[0])/19\n",
    "        except:\n",
    "            original_te, original_p_value = np.nan, np.nan\n",
    "            \n",
    "        # No nonnegativity constraint\n",
    "        try:\n",
    "            nonn_gap_dict = placebo_weight_and_gap_computer(\n",
    "                data_stationary, SyntheticControl(\n",
    "                    verbose_inner=False, verbose_outer=False, weight_nonnegative_restr=False, weight_sum_restr=True))\n",
    "            nonn_te = np.round(np.sum(nonn_gap_dict['A'][(T0+1):])/45, 2)\n",
    "            nonn_p_value = (20 - (pd.Series(ratiofit_pre_post(nonn_gap_dict).values()).rank())[0])/19\n",
    "        except:\n",
    "            nonn_te, nonn_p_value = np.nan, np.nan\n",
    "        \n",
    "        # No sum constraint\n",
    "        try:\n",
    "            nosum_gap_dict = placebo_weight_and_gap_computer(\n",
    "                data_stationary, SyntheticControl(\n",
    "                    verbose_inner=False, verbose_outer=False, weight_nonnegative_restr=True, weight_sum_restr=False))\n",
    "            nosum_te = np.round(np.sum(nosum_gap_dict['A'][(T0+1):])/45, 2)\n",
    "            nosum_p_value = (20 - (pd.Series(ratiofit_pre_post(nosum_gap_dict).values()).rank())[0])/19\n",
    "        except:\n",
    "            nosum_te, nosum_p_value = np.nan, np.nan\n",
    "\n",
    "        # Unconstrained\n",
    "        try:\n",
    "            unconstr_gap_dict = placebo_weight_and_gap_computer(\n",
    "                data_stationary, SyntheticControl(\n",
    "                    verbose_inner=False, verbose_outer=False, weight_nonnegative_restr=False, weight_sum_restr=False))\n",
    "            unconstr_te = np.round(np.sum(unconstr_gap_dict['A'][(T0+1):])/45, 2)\n",
    "            unconstr_p_value = (20 - (pd.Series(ratiofit_pre_post(unconstr_gap_dict).values()).rank())[0])/19\n",
    "        except:\n",
    "            unconstr_te, unconstr_p_value = np.nan, np.nan\n",
    "\n",
    "        ###### Nonstationary data\n",
    "        # Original method\n",
    "        try:\n",
    "            N_original_gap_dict = placebo_weight_and_gap_computer(\n",
    "                data_nonstationary, SyntheticControl(\n",
    "                    verbose_inner=False, verbose_outer=False, weight_nonnegative_restr=True, weight_sum_restr=True))\n",
    "            N_original_te = np.round(np.sum(N_original_gap_dict['A'][(T0+1):])/45, 2)\n",
    "            N_original_p_value = (20 - (pd.Series(ratiofit_pre_post(N_original_gap_dict).values()).rank())[0])/19\n",
    "        except:\n",
    "            N_original_te, N_original_p_value = np.nan, np.nan\n",
    "        \n",
    "        # No nonnegativity constraint\n",
    "        try:\n",
    "            N_nonn_gap_dict = placebo_weight_and_gap_computer(\n",
    "                data_nonstationary, SyntheticControl(\n",
    "                    verbose_inner=False, verbose_outer=False, weight_nonnegative_restr=False, weight_sum_restr=True))\n",
    "            N_nonn_te = np.round(np.sum(N_nonn_gap_dict['A'][(T0+1):])/45, 2)\n",
    "            N_nonn_p_value = (20 - (pd.Series(ratiofit_pre_post(N_nonn_gap_dict).values()).rank())[0])/19\n",
    "        except:\n",
    "            N_nonn_te, N_nonn_p_value = np.nan, np.nan\n",
    "        \n",
    "        # No sum constraint\n",
    "        try:\n",
    "            N_nosum_gap_dict = placebo_weight_and_gap_computer(\n",
    "                data_nonstationary, SyntheticControl(\n",
    "                    verbose_inner=False, verbose_outer=False, weight_nonnegative_restr=True, weight_sum_restr=False))\n",
    "            N_nosum_te = np.round(np.sum(N_nosum_gap_dict['A'][(T0+1):])/45, 2)\n",
    "            N_nosum_p_value = (20 - (pd.Series(ratiofit_pre_post(N_nosum_gap_dict).values()).rank())[0])/19\n",
    "        except:\n",
    "            N_nosum_te, N_nosum_p_value = np.nan, np.nan\n",
    "\n",
    "        # Unconstrained\n",
    "        try:\n",
    "            N_unconstr_gap_dict = placebo_weight_and_gap_computer(\n",
    "                data_nonstationary, SyntheticControl(\n",
    "                    verbose_inner=False, verbose_outer=False, weight_nonnegative_restr=False, weight_sum_restr=False))\n",
    "            N_unconstr_te = np.round(np.sum(N_unconstr_gap_dict['A'][(T0+1):])/45, 2)\n",
    "            N_unconstr_p_value = (20 - (pd.Series(ratiofit_pre_post(N_unconstr_gap_dict).values()).rank())[0])/19\n",
    "        except:\n",
    "            N_unconstr_te, N_unconstr_p_value = np.nan, np.nan\n",
    "        \n",
    "        # Store TE and p-values\n",
    "        W_results.append({'Simulation': sim_no+1, 'TE_size': TE_size,\n",
    "                          'O TE': original_te, 'O p-val':original_p_value,\n",
    "                          'NN TE': nonn_te, 'NN p-val':nonn_p_value,\n",
    "                          'S TE': nosum_te, 'S p-val':nosum_p_value,\n",
    "                          'U TE': unconstr_te, 'U p-val':unconstr_p_value,\n",
    "                          'N O TE': N_original_te, 'N O p-val':N_original_p_value,\n",
    "                          'N NN TE': N_nonn_te, 'N NN p-val':N_nonn_p_value,\n",
    "                          'N S TE': N_nosum_te, 'N S p-val':N_nosum_p_value,\n",
    "                          'N U TE': N_unconstr_te, 'N U p-val':N_unconstr_p_value\n",
    "                       })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f00f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_W_results = pd.DataFrame(W_results)\n",
    "np.round(df_W_results.groupby(['TE_size']).mean().drop('Simulation', axis=1), 3).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b4a2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_W_to_latex = np.round(df_W_results.groupby(['TE_size']).mean().drop('Simulation', axis=1),3).T\n",
    "print(df_W_to_latex.style.to_latex())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
